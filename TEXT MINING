from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report
# Sample data
data = [
 ("I love programming in Python", "positive"),
 ("Python is the best language for data science", "positive"),
 ("I hate bugs in my code", "negative"),
 ("Debugging Python code is frustrating", "negative"),
 ("Data science is a fascinating field", "positive"),
 ("I don't like working with Java", "negative")
]
# Prepare Data
texts, labels = zip(*data)
# Split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.3,
random_state=42)
# Vectorize text using TF-IDF
vectorizer = TfidfVectorizer(stop_words='english')
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)
# Train a Naive Bayes classifier
model = MultinomialNB()
model.fit(X_train_tfidf, y_train)
# Predict on the test set
y_pred = model.predict(X_test_tfidf)
# Evaluate model
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:")
print(classification_report(y_test, y_pred, zero_division=1))
# Predict a new text
new_text = ["I enjoy coding in Python"]
new_text_tfidf = vectorizer.transform(new_text)
prediction = model.predict(new_text_tfidf)
print(f"Predicted label for new text: {prediction[0]}")
